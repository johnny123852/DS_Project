{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"test\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=None, txid=None, vout_txid=None, vout_n=None, address=None, value=None, blockhash=None, blocktime=None, vout_type=None, vout_script=None, tx_index=None, block_reward=None)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"txid\", StringType(), True),\n",
    "    StructField(\"vout_txid\", StringType(), True),\n",
    "    StructField(\"vout_n\", IntegerType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"value\", FloatType(), True),\n",
    "    StructField(\"blockhash\", StringType(), True),\n",
    "    StructField(\"blocktime\", StringType(), True),\n",
    "    StructField(\"vout_type\", StringType(), True),\n",
    "    StructField(\"vout_script\", StringType(), True),\n",
    "    StructField(\"tx_index\", StringType(), True),\n",
    "    StructField(\"block_reward\", StringType(), True)])\n",
    "\n",
    "df_raw_out = spark.read.load(\"gs://bucket-1-btc/*-txOut.csv\", format=\"csv\", delimiter=\"\\t\", header=False, schema=schema)\n",
    "df_raw_out.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18866209"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sum transation,總交易數\n",
    "df_raw_out.createOrReplaceTempView(\"table1\")\n",
    "dis_txid = spark.sql(\"SELECT distinct txid from table1\")\n",
    "#dis_txid.collect()\n",
    "\n",
    "dis_txid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18866209"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sum transation,總交易數\n",
    "#用RDD方式\n",
    "rdd = df_raw_out.select(\"txid\").rdd\n",
    "rdd.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(blockhash='000000000000000000084b32d266e4a728eac41e46e36891712e31ccf78d0485', txidnum=3824)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one block max transation,一個block最大交易數\n",
    "bloc_txid = spark.sql(\"select blockhash, count(txid) as txidnum from \"+\n",
    "                      \"(SELECT distinct blockhash, txid from table1) as table2 \" + \n",
    "                      \"group by blockhash order by count(txid) DESC\")\n",
    "#bloc_txid.collect()\n",
    "\n",
    "#bloc_txid.show()\n",
    "bloc_txid.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sum(value)=94755445.49863167)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total value,總交易金額\n",
    "txidcount = spark.sql(\"select sum(value) from table1\")\n",
    "#txidcount.collect()\n",
    "\n",
    "txidcount.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(blockhash='000000000000000000133d8b3b6425d4bf686aad50a727a9ee351972828d5c59', totalvalue=155929.94302857365)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one block max total value,一個block最大交易金額\n",
    "bloc_totalvalue = spark.sql(\"SELECT blockhash, sum(value) as totalvalue from table1 \" + \n",
    "                      \"group by blockhash order by sum(value) DESC\")\n",
    "#bloc_totalvalue.collect()\n",
    "\n",
    "#bloc_txid.show()\n",
    "bloc_totalvalue.head(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}